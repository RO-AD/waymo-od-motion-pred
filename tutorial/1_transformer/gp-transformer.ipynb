{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7Mq7M/F+GGw7W2qdfdCFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RO-AD/waymo-od-motion-pred/blob/main/tutorial/1_transformer/gp-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer 실습\n",
        "참고 : [Attention is All You Need Tutorial(ndb796)](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb)\n",
        "\n",
        "자연어처리 끝판왕. 다른 분야에도 고르게 사용됨."
      ],
      "metadata": {
        "id": "Yzlf9NcHRx-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아,,, colab에서 UTF-8 오류 뜨는데 아래 코드로 해결함,,, 찝찝,,,,하네\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "x3eIgf7XRq8B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자연어를 어떻게 Input으로 줄 수 있을까?\n",
        "기본적으로 모델의 INPUT은 숫자 형태로 들어가기 때문에 처리하고자 하는 문장을 분해 및 숫자로 변환하는 전처리 작업이 필요하다. 이 과정을 문장의 토큰화(tokenization)이라고 한다.\n",
        "\n",
        "**torchtext**는 자연어처리에 유용한 라이브러리로, 데이터셋과 전처리 클래스, 스코어링 함수 등이 존재한다. 설치해보자!"
      ],
      "metadata": {
        "id": "BVf0y93oS3WD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZV6L3KLCF4Iu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 **spaCy** 라이브러리를 이용해보자. **토큰화(tokenization), 태깅(tagging) 등의 전처리**를 위한 라이브러리다. 영어와 한국어 전처리 모듈을 설치하고 확인해보자"
      ],
      "metadata": {
        "id": "RjoiP6KPVF6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm"
      ],
      "metadata": {
        "id": "99OQurMnTyp3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
        "spacy_ko = spacy.load('ko_core_news_sm') # 한국어 토큰화(tokenization)"
      ],
      "metadata": {
        "id": "_WcabdT8VwUu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "_tmp_en_tokens = spacy_en.tokenizer(\"I graduated last week.\")\n",
        "_tmp_ko_tokens = spacy_ko.tokenizer(\"저는 저번 주에 졸업했습니다.\")\n",
        "\n",
        "# 영어 문장 토큰화\n",
        "for i, token in enumerate(_tmp_en_tokens):\n",
        "    print(f\"인덱스 {i}: {token.text}\")\n",
        "\n",
        "print(\"--------------\")\n",
        "\n",
        "# 한국어 문장 토큰화\n",
        "for i, token in enumerate(_tmp_ko_tokens):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ],
      "metadata": {
        "id": "9RYjNqBeW2qH",
        "outputId": "0b0c55c9-c2d8-4313-a40b-91b791469b4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: graduated\n",
            "인덱스 2: last\n",
            "인덱스 3: week\n",
            "인덱스 4: .\n",
            "--------------\n",
            "인덱스 0: 저는\n",
            "인덱스 1: 저번\n",
            "인덱스 2: 주에\n",
            "인덱스 3: 졸업했습니다\n",
            "인덱스 4: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화 과정을 함수화 해보자"
      ],
      "metadata": {
        "id": "oBS1FTYGX7nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = lambda text: [token.text for token in spacy_en.tokenizer(text)]\n",
        "tokenizer_ko = lambda text: [token.text for token in spacy_ko.tokenizer(text)]"
      ],
      "metadata": {
        "id": "SR4CVOO8XfH5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en('I graduated last week.')"
      ],
      "metadata": {
        "id": "HK3S4Y-nYPjM",
        "outputId": "b5a3942d-f8b1-485f-bdb0-360ebb6aad0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'graduated', 'last', 'week', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필드(field)를 이용해 데이터셋에 대한 구체적인 전처리 과정을 명시할 수 있다. 이 작업을 통해 추후 코드에서 전처리를 조금 더 단순하게 다룰 수 있다."
      ],
      "metadata": {
        "id": "pxEBfimqY2A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "# 목표 : ko -> en 로 번역\n",
        "src_field = Field(tokenize=tokenizer_ko, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n",
        "trg_field = Field(tokenize=tokenizer_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"
      ],
      "metadata": {
        "id": "RrUd3OiRYTI7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter**\n",
        "- tokenize : 어떤 토큰화 함수를 적용할 것인지 지정(default : split)\n",
        "- init_token : ?\n",
        "- eos_token : ?\n",
        "- lower : 대문자가 있다면 소문자로 변경할 것인지\n",
        "- batch_first : ?(미니배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지)"
      ],
      "metadata": {
        "id": "GWYQxmJHZ2Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 데이터셋을 불러오자. **torchtext의 datasets**에는 대표적인 번역 데이터셋인 **Mullti30k**이 있다. 였는데,,, 아쉽게도 한국어는 없어서 다른 라이브러리를 사용함.\n",
        "\n",
        "여러 데이터셋을 모아서 라이브러리화 하는 **Korpora**라는 라이브러리가 있었음"
      ],
      "metadata": {
        "id": "rwvn4ltVZvf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install Korpora"
      ],
      "metadata": {
        "id": "OR1DWDxUhl7x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "Korpora.fetch(\"korean_parallel_koen_news\")\n",
        "datasets = Korpora.load(\"korean_parallel_koen_news\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lS2pfM5iULU",
        "outputId": "7ce37774-e332-4392-e7d9-ca518e432375"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Korpora] Corpus `korean_parallel` is already installed at /root/Korpora/korean_parallel/korean-english-park.train.tar.gz\n",
            "decompress /root/Korpora/korean_parallel/korean-english-park.train.tar.gz\n",
            "[Korpora] Corpus `korean_parallel` is already installed at /root/Korpora/korean_parallel/korean-english-park.dev.tar.gz\n",
            "decompress /root/Korpora/korean_parallel/korean-english-park.dev.tar.gz\n",
            "[Korpora] Corpus `korean_parallel` is already installed at /root/Korpora/korean_parallel/korean-english-park.test.tar.gz\n",
            "decompress /root/Korpora/korean_parallel/korean-english-park.test.tar.gz\n",
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : KakaoBrain\n",
            "    Repository : https://github.com/jungyeul/korean-parallel-corpora\n",
            "    References :\n",
            "        - Jungyeul Park, Jeen-Pyo Hong and Jeong-Won Cha (2016) Korean Language Resources for Everyone.\n",
            "          In Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation\n",
            "          (PACLIC 30). October 28 - 30, 2016. Seoul, Korea. \n",
            "          (https://www.aclweb.org/anthology/Y16-2002/)\n",
            "\n",
            "    # License\n",
            "    Creative Commons Attribution Noncommercial No-Derivative-Works 3.0\n",
            "    Details in https://creativecommons.org/licenses/by-nc-nd/3.0/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wV4QbSDi_-g",
        "outputId": "e11c94da-9fe3-46b9-e84e-7099c6e6b34b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "koennews.train: size=94123\n",
              "  - koennews.train.texts : list[str]\n",
              "  - koennews.train.pairs : list[str]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RN3iiZWjBSQ",
        "outputId": "7d2dea95-4f84-47ec-c10a-758020921db0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "koennews.test: size=2000\n",
              "  - koennews.test.texts : list[str]\n",
              "  - koennews.test.pairs : list[str]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불러온 데이터셋을 예제를 따라가기 편한 구조로 변경해주었다. 기존의 `torchtext.datasets.translation.Multi30k` 구조가 `torchtext.data.Example`의 집합을 `torchtext.data.Dataset`으로 묶어준 형태여서 이것을 그대로 재현했다."
      ],
      "metadata": {
        "id": "7Ye8t-EFVKij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Example\n",
        "\n",
        "test_dataset_examples = []\n",
        "\n",
        "for text, pair in zip(datasets.test.texts, datasets.test.pairs):\n",
        "  example = Example.fromlist([text, pair], [(\"src\", src_field), (\"trg\", trg_field)])\n",
        "  test_dataset_examples.append(example)\n",
        "\n",
        "train_dataset_examples = []\n",
        "\n",
        "for text, pair in zip(datasets.test.texts, datasets.test.pairs):\n",
        "  example = Example.fromlist([text, pair], [(\"src\", src_field), (\"trg\", trg_field)])\n",
        "  train_dataset_examples.append(example)"
      ],
      "metadata": {
        "id": "ogSIV47d3JsH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Dataset\n",
        "\n",
        "train_dataset = Dataset(examples=train_dataset_examples, fields=[(\"src\", src_field), (\"trg\", trg_field)])\n",
        "test_dataset  = Dataset(examples=test_dataset_examples,  fields=[(\"src\", src_field), (\"trg\", trg_field)])"
      ],
      "metadata": {
        "id": "6CYNdaKGUNG9"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 중 하나를 선택해 출력\n",
        "print(vars(train_dataset.examples[30])['src'])\n",
        "print(vars(train_dataset.examples[30])['trg'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1pM5_HGWfp_",
        "outputId": "70be447a-0ecf-4b5d-a81c-6cd8ec52e094"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이', '박테리아들은', '로켓이나', '위성에', '실려', '우주로', '갔을', '수도', '있으며', ',', '아니면', '정말로', '다른', '행성에서', '온', '것일', '수도', '있다', '.']\n",
            "['the', 'bacteria', 'could', 'have', 'hitched', 'a', 'ride', 'on', 'a', 'rocket', 'or', 'satellite', 'into', 'space', 'or', 'they', 'really', 'could', 'be', 'from', 'another', 'planet', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 토큰화된 dataset을 구비했다. 이제 각 단어를 숫자 형태로 변경해주어야 한다. **Field**의 **build_vocab** 메서드를 이용해 사전을 만들 수 있고, 이를 기반으로 숫자로 변경한다."
      ],
      "metadata": {
        "id": "wgtyItSZVvbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_field.build_vocab(train_dataset, min_freq=2)\n",
        "trg_field.build_vocab(train_dataset, min_freq=2)\n",
        "\n",
        "print(f\"src vocab len : {len(src_field.vocab)}\")\n",
        "print(f\"trg vocab len : {len(trg_field.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHjlWc9iUga-",
        "outputId": "a01fda06-0cf0-47fb-9c29-8668393ba944"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src vocab len : 4162\n",
            "trg vocab len : 4020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter**\n",
        "- min_freq : 최소 2번 이상 등장한 단어"
      ],
      "metadata": {
        "id": "Dgm6ljYzX368"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trg_field.vocab.itos[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d1EEWGmfiwAi",
        "outputId": "f4ca2509-1561-406e-aef9-0790fb1915da"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trg_field.vocab.stoi[\"hihihihi\"]) # 없는 단어: 0\n",
        "print(trg_field.vocab.stoi[trg_field.pad_token]) # 패딩(padding): 1\n",
        "print(trg_field.vocab.stoi[\"<sos>\"]) # : 2\n",
        "print(trg_field.vocab.stoi[\"<eos>\"]) # : 3\n",
        "print(trg_field.vocab.stoi[\"a\"])\n",
        "print(trg_field.vocab.stoi[\"world\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az8LsQzfUzNM",
        "outputId": "c20c9749-9b04-4826-a1fb-7b209087abb8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "8\n",
            "93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab 내에서 각 데이터를 숫자로 변환할 수 있는 것을 확인할 수 있었다. 그러나 각 문장 내 단어의 개수가 각기 다르기 때문에 BucketIterator를 이용하여 패딩을 포함한 동일한 개수로 맞춰줄 수 있다.\n",
        "- 배치 크기(batch size): 128"
      ],
      "metadata": {
        "id": "tyjNQeciU1wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import BucketIterator\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# 일반적인 데이터 로더(data loader)의 iterator와 유사하게 사용 가능\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, test_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device='cuda')"
      ],
      "metadata": {
        "id": "gYAcPpjlWxCL"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
        "\n",
        "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
        "    for i in range(src.shape[1]):\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
        "\n",
        "    # 첫 번째 배치만 확인\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99kR6lincGRe",
        "outputId": "fe6d54e7-4b7e-4e63-8a53-e911fb242c04"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 배치 크기: torch.Size([128, 42])\n",
            "인덱스 0: 2\n",
            "인덱스 1: 92\n",
            "인덱스 2: 1381\n",
            "인덱스 3: 334\n",
            "인덱스 4: 0\n",
            "인덱스 5: 0\n",
            "인덱스 6: 598\n",
            "인덱스 7: 0\n",
            "인덱스 8: 659\n",
            "인덱스 9: 3463\n",
            "인덱스 10: 904\n",
            "인덱스 11: 125\n",
            "인덱스 12: 15\n",
            "인덱스 13: 4\n",
            "인덱스 14: 3\n",
            "인덱스 15: 1\n",
            "인덱스 16: 1\n",
            "인덱스 17: 1\n",
            "인덱스 18: 1\n",
            "인덱스 19: 1\n",
            "인덱스 20: 1\n",
            "인덱스 21: 1\n",
            "인덱스 22: 1\n",
            "인덱스 23: 1\n",
            "인덱스 24: 1\n",
            "인덱스 25: 1\n",
            "인덱스 26: 1\n",
            "인덱스 27: 1\n",
            "인덱스 28: 1\n",
            "인덱스 29: 1\n",
            "인덱스 30: 1\n",
            "인덱스 31: 1\n",
            "인덱스 32: 1\n",
            "인덱스 33: 1\n",
            "인덱스 34: 1\n",
            "인덱스 35: 1\n",
            "인덱스 36: 1\n",
            "인덱스 37: 1\n",
            "인덱스 38: 1\n",
            "인덱스 39: 1\n",
            "인덱스 40: 1\n",
            "인덱스 41: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeYlpwLHcRET"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}