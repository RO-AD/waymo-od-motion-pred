{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5H5Z6+jIAzLKaWGx34Amp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RO-AD/waymo-od-motion-pred/blob/main/tutorial/1_transformer/gp-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer 실습\n",
        "참고 : [Attention is All You Need Tutorial(ndb796)](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb)\n",
        "\n",
        "자연어처리 끝판왕. 다른 분야에도 고르게 사용됨."
      ],
      "metadata": {
        "id": "Yzlf9NcHRx-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자연어를 어떻게 Input으로 줄 수 있을까?\n",
        "기본적으로 모델의 INPUT은 숫자 형태로 들어가기 때문에 처리하고자 하는 문장을 분해 및 숫자로 변환하는 전처리 작업이 필요하다. 이 과정을 문장의 토큰화(tokenization)이라고 한다.\n",
        "\n",
        "**torchtext**는 자연어처리에 유용한 라이브러리로, 데이터셋과 전처리 클래스, 스코어링 함수 등이 존재한다. 설치해보자!"
      ],
      "metadata": {
        "id": "BVf0y93oS3WD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZV6L3KLCF4Iu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "637f33c4-1105-4022-8fc4-08494b01db4c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a300fc016bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torchtext==0.6.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    437\u001b[0m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    164\u001b[0m         'A UTF-8 locale is required. Got {}'.format(locale_encoding))\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 **spaCy** 라이브러리를 이용해보자. **토큰화(tokenization), 태깅(tagging) 등의 전처리**를 위한 라이브러리다. 영어와 한국어 전처리 모듈을 설치하고 확인해보자"
      ],
      "metadata": {
        "id": "RjoiP6KPVF6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm"
      ],
      "metadata": {
        "id": "99OQurMnTyp3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "5a799109-8a65-4fd9-a213-68248742d99b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d4bcb3809e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download ko_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    437\u001b[0m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    164\u001b[0m         'A UTF-8 locale is required. Got {}'.format(locale_encoding))\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
        "spacy_ko = spacy.load('ko_core_news_sm') # 한국어 토큰화(tokenization)"
      ],
      "metadata": {
        "id": "_WcabdT8VwUu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단히 토큰화(tokenization) 기능 써보기\n",
        "_tmp_en_tokens = spacy_en.tokenizer(\"I graduated last week.\")\n",
        "_tmp_ko_tokens = spacy_ko.tokenizer(\"저는 저번 주에 졸업했습니다.\")\n",
        "\n",
        "# 영어 문장 토큰화\n",
        "for i, token in enumerate(_tmp_en_tokens):\n",
        "    print(f\"인덱스 {i}: {token.text}\")\n",
        "\n",
        "print(\"--------------\")\n",
        "\n",
        "# 한국어 문장 토큰화\n",
        "for i, token in enumerate(_tmp_ko_tokens):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ],
      "metadata": {
        "id": "9RYjNqBeW2qH",
        "outputId": "c5d29afc-0679-4c8c-b4cd-65f93a45d407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: graduated\n",
            "인덱스 2: last\n",
            "인덱스 3: week\n",
            "인덱스 4: .\n",
            "--------------\n",
            "인덱스 0: 저는\n",
            "인덱스 1: 저번\n",
            "인덱스 2: 주에\n",
            "인덱스 3: 졸업했습니다\n",
            "인덱스 4: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화 과정을 함수화 해보자"
      ],
      "metadata": {
        "id": "oBS1FTYGX7nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = lambda text: [token.text for token in spacy_en.tokenizer(text)]\n",
        "tokenizer_ko = lambda text: [token.text for token in spacy_ko.tokenizer(text)]"
      ],
      "metadata": {
        "id": "SR4CVOO8XfH5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en('I graduated last week.')"
      ],
      "metadata": {
        "id": "HK3S4Y-nYPjM",
        "outputId": "0c950483-6527-4600-8480-462e3b51b4eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'graduated', 'last', 'week', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필드(field)를 이용해 데이터셋에 대한 구체적인 전처리 과정을 명시할 수 있다. 이 작업을 통해 추후 코드에서 전처리를 조금 더 단순하게 다룰 수 있다."
      ],
      "metadata": {
        "id": "pxEBfimqY2A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "# 목표 : ko -> en 로 번역\n",
        "SRC = Field(tokenize=tokenizer_ko, init_token=\"\", eos_token=\"\", lower=True, batch_first=True)\n",
        "DST = Field(tokenize=tokenizer_en, init_token=\"\", eos_token=\"\", lower=True, batch_first=True)"
      ],
      "metadata": {
        "id": "RrUd3OiRYTI7"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter**\n",
        "- tokenize : 어떤 토큰화 함수를 적용할 것인지 지정(default : split)\n",
        "- init_token : ?\n",
        "- eos_token : ?\n",
        "- lower : 대문자가 있다면 소문자로 변경할 것인지\n",
        "- batch_first : ?(미니배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지)"
      ],
      "metadata": {
        "id": "GWYQxmJHZ2Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 데이터셋을 불러오자. **torchtext의 datasets**에는 대표적인 번역 데이터셋인 **Mullti30k**이 있다. 였는데,,, 아쉽게도 한국어는 없어서 다른 라이브러리를 사용함.\n",
        "\n",
        "여러 데이터셋을 모아서 라이브러리화 하는 **Korpora**라는 라이브러리가 있었음"
      ],
      "metadata": {
        "id": "rwvn4ltVZvf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아,,, colab에서 UTF-8 오류 뜨는데 아래 코드로 해결함,,, 찝찝,,,,하네\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "yeJAhI_UhkaZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install Korpora"
      ],
      "metadata": {
        "id": "OR1DWDxUhl7x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "Korpora.fetch(\"korean_parallel_koen_news\")\n",
        "datasets = Korpora.load(\"korean_parallel_koen_news\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lS2pfM5iULU",
        "outputId": "f4b27b59-6eca-410d-e25f-e78752220962"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Korpora] Corpus `korean_parallel` is already installed at /root/Korpora/korean_parallel/korean-english-park.train.tar.gz\n",
            "decompress /root/Korpora/korean_parallel/korean-english-park.train.tar.gz\n",
            "[Korpora] Corpus `korean_parallel` is already installed at /root/Korpora/korean_parallel/korean-english-park.dev.tar.gz\n",
            "decompress /root/Korpora/korean_parallel/korean-english-park.dev.tar.gz\n",
            "[Korpora] Corpus `korean_parallel` is already installed at /root/Korpora/korean_parallel/korean-english-park.test.tar.gz\n",
            "decompress /root/Korpora/korean_parallel/korean-english-park.test.tar.gz\n",
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : KakaoBrain\n",
            "    Repository : https://github.com/jungyeul/korean-parallel-corpora\n",
            "    References :\n",
            "        - Jungyeul Park, Jeen-Pyo Hong and Jeong-Won Cha (2016) Korean Language Resources for Everyone.\n",
            "          In Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation\n",
            "          (PACLIC 30). October 28 - 30, 2016. Seoul, Korea. \n",
            "          (https://www.aclweb.org/anthology/Y16-2002/)\n",
            "\n",
            "    # License\n",
            "    Creative Commons Attribution Noncommercial No-Derivative-Works 3.0\n",
            "    Details in https://creativecommons.org/licenses/by-nc-nd/3.0/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wV4QbSDi_-g",
        "outputId": "10bf9588-a3b7-4834-ab4f-2da3f84d1332"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "koennews.train: size=94123\n",
              "  - koennews.train.texts : list[str]\n",
              "  - koennews.train.pairs : list[str]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RN3iiZWjBSQ",
        "outputId": "03a709db-fb17-413d-b8bb-13fa9092ed72"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "koennews.test: size=2000\n",
              "  - koennews.test.texts : list[str]\n",
              "  - koennews.test.pairs : list[str]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "torchtext.datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm0qBkaxne9I",
        "outputId": "3b72661d-1386-43a6-971f-fc313cc8c45f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'torchtext.datasets' from '/usr/local/lib/python3.8/dist-packages/torchtext/datasets/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = TabularDataset.splits(\n",
        "        path='.', train=datasets.train, test=datasets.test, format=,\n",
        "        fields=[('text', SRC), ('label', DST)], skip_header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srRRHaEioHnp",
        "outputId": "056af788-f0da-4137-852e-dbcfe23380bb"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__name__',\n",
              " '__doc__',\n",
              " '__package__',\n",
              " '__loader__',\n",
              " '__spec__',\n",
              " '__path__',\n",
              " '__file__',\n",
              " '__cached__',\n",
              " '__builtins__',\n",
              " 'utils',\n",
              " 'vocab',\n",
              " 'data',\n",
              " 'datasets',\n",
              " 'experimental',\n",
              " '__version__',\n",
              " '__all__']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchtext.data.Dataset(\n",
        "                    examples=[datasets.train.texts, datasets.train.pairs],\n",
        "                    fields=[('src', SRC), ('trg', DST)])\n",
        "test_dataset  = torchtext.data.Dataset(\n",
        "                    examples=[datasets.test.texts, datasets.test.pairs],\n",
        "                    fields=[('src', SRC), ('trg', DST)])"
      ],
      "metadata": {
        "id": "HLdIPf93oLbh"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = torchtext.data.Dataset(\n",
        "                    examples=datasets.train.texts,\n",
        "                    fields=[('src', SRC)])"
      ],
      "metadata": {
        "id": "GAtcqzW3oRow"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.examples[20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "CMA34yYDqMZ0",
        "outputId": "2daea1a7-f42f-4cf5-be6b-0be0a5a7f507"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchtext.data.TabularDataset()"
      ],
      "metadata": {
        "id": "bTy5edKhqiOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}