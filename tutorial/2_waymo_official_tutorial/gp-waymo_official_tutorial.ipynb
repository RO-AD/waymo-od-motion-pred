{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TCYsCkYExbSy",
        "AZDQWzC3uuU0",
        "ZHc3XBhD3jog",
        "iMDBpCY-wje5",
        "xEqkgmbf465P",
        "RYs2WnudIb1Y",
        "en6NgFe8BFbm",
        "fQQu3WZc9y8-"
      ],
      "authorship_tag": "ABX9TyNPnHMJt73tNTDOVekX8zBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RO-AD/waymo-od-motion-pred/blob/main/tutorial/2_waymo_official_tutorial/gp-waymo_official_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Waymo 공식 튜토리얼 실습\n",
        "\n",
        "공식 튜토리얼이 오류가 좀 있어서 과거의 나에게 힘을 좀 빌렸다. 내맘대로 바꿔버리는 튜토리얼,,,ㅋ\n",
        "\n",
        "- Waymo 공식 튜토리얼 : https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial_motion.ipynb\n",
        "\n",
        "- 과거의 재승 : https://github.com/RO-AD/waymo-od-motion-pred/blob/b0b831d8a54531056289b85fb91b509a78bd1a20/jaeseung/tutorial-1.ipynb"
      ],
      "metadata": {
        "id": "i5xL796pv44P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 설치"
      ],
      "metadata": {
        "id": "106qkW5GwwGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## waymo_open_dataset 패키지 설치하기"
      ],
      "metadata": {
        "id": "TCYsCkYExbSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -rf waymo-od > /dev/null # 레포가 이미 존재하면 지움\n",
        "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
        "!cd waymo-od && git checkout remotes/origin/master"
      ],
      "metadata": {
        "id": "bdkrsThHwvyS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install --upgrade pip\n",
        "!pip3 install waymo-open-dataset-tf-2-6-0\n",
        "!pip install keras==2.6 # 튜토리얼에 맞게 케라스 버전 맞춰주어야 함"
      ],
      "metadata": {
        "id": "v1mqUFXxxpWc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read one frame\n",
        "하나의 프레임을 읽어보자. 파일이 어떻게 생겼고, 어떻게 읽을 수 있는지 알아본다"
      ],
      "metadata": {
        "id": "AZDQWzC3uuU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al ./waymo-od/tutorial/frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czxnHAOuzjth",
        "outputId": "f49c1334-ba5d-4929-dfd9-f00912345b9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 8577921 Apr 11 09:51 ./waymo-od/tutorial/frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!xxd ./waymo-od/tutorial/frames | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_ILGOQ-zayC",
        "outputId": "2dfaf53d-70eb-4b61-a72f-f3ca9653db66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00000000: ae5a 4100 0000 0000 e0a5 4a4b 0aa8 150a  .ZA.......JK....\n",
            "00000010: 2631 3130 3034 3638 3537 3339 3731 3435  &110046857397145\n",
            "00000020: 3030 3232 305f 3233 3030 5f30 3030 5f32  00220_2300_000_2\n",
            "00000030: 3332 305f 3030 3012 ee01 0801 11dd ef9b  320_000.........\n",
            "00000040: bf1c 0fa0 4011 ddef 9bbf 1c0f a040 11d8  ....@........@..\n",
            "00000050: 5892 7f42 5d8d 4011 e81f 42d4 9308 8440  X..B].@...B....@\n",
            "00000060: 118f b145 4fb9 8ba0 3f11 41c3 45a9 0592  ...EO...?.A.E...\n",
            "00000070: d4bf 11ba 4ac5 6e55 fe49 3f11 85b3 7881  ....J.nU.I?...x.\n",
            "00000080: 2a81 44bf 1100 0000 0000 0000 001a 9001  *.D.............\n",
            "00000090: 0935 29b1 f11e ffef 3f09 0c0a 38a1 558c  .5).....?...8.U.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# Eager Execution 모드 활성화 -> 디버깅 용이 및 코드 실행속도 향상\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "from waymo_open_dataset import dataset_pb2 # pb : protocol buffers\n",
        "\n",
        "FILENAME = '/content/waymo-od/tutorial/frames' # git repo 내에 있음\n",
        "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
        "for data in dataset:\n",
        "    frame = dataset_pb2.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    break"
      ],
      "metadata": {
        "id": "Pfd_Yhpdz1RK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from waymo_open_dataset.utils import  frame_utils\n",
        "\n",
        "(range_images, camera_projections, _, range_image_top_pose)\\\n",
        "  = frame_utils.parse_range_image_and_camera_projection(frame)"
      ],
      "metadata": {
        "id": "36QEVH9H1RIf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 로드"
      ],
      "metadata": {
        "id": "ZHc3XBhD3jog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "클라우드에 저장되어있는 데이터셋을 확인해보면 위와 같이 크게 세 종류로 구성되어있는데, lidar는 올해 새로 생긴 라이다 데이터이며, scenario와 tf_example이 map, agent 등 다양한 데이터가 담긴 데이터셋이다.\n",
        "\n",
        "scenario는 라이브러리 내에 선언되어있는 proto 구조로 파싱 가능한 데이터이며, tf_example은 tf에서 다루기 좋은 구조로 이루어진 데이터다. 동일한 구조라고 한다.\n",
        "\n",
        "튜토리얼은 tf_example 데이터로 진행되기 때문에, 이 데이터를 사용한다.\n",
        "\n",
        "*처음에 scenario 데이터로 하다가 수많은 오류와 맞이하여 미친듯이 삽질을 했다는,,,아래 이슈들을 통해 오류를 해결해나갔다.*\n",
        "- https://github.com/waymo-research/waymo-open-dataset/issues/258\n",
        "  - 파싱이 안된다는 이슈.\n",
        "  - tf_example 데이터셋을 사용하라는 답변.\n",
        "- https://github.com/waymo-research/waymo-open-dataset/issues/617\n",
        "  - tf_example로 했는데도 파싱이 안되는 이슈\n",
        "  - roadgraph_samples의 최대 포인트 수가 잘못됨. 20k -> 30k"
      ],
      "metadata": {
        "id": "nNMVHV3DN4hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive 연동\n",
        "일단 테스트용도로 validation 5개를 drive에 업로드 했다. 이를 가져온다."
      ],
      "metadata": {
        "id": "iMDBpCY-wje5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZOhGHfuueC",
        "outputId": "8fcfc032-507f-444c-cafc-2aeb7ec671a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/waymo-od-dataset/waymo_open_dataset_motion_v_1_2_0/uncompressed/tf_example ./"
      ],
      "metadata": {
        "id": "asD5_6zeuub0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al ./tf_example/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByfoT3uuuuZg",
        "outputId": "24a392be-02a6-4f3d-98d8-08a23c9550fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 583836\n",
            "drwx------ 2 root root      4096 Apr 11 09:54 .\n",
            "drwxr-xr-x 1 root root      4096 Apr 11 09:54 ..\n",
            "-rw------- 1 root root 597834382 Apr 11 09:54 uncompressed_tf_example_validation_validation_tfexample.tfrecord-00000-of-00150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## python 코드로 데이터셋 읽기"
      ],
      "metadata": {
        "id": "xEqkgmbf465P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋의 양이 너무 많고 크기 때문에, 데이터셋도 효율적으로 관리할 필요가 있다. Tensorflow는 TFRecord라는 형태의 파일로 관리할 수 있도록 하는데, 구조를 최소한으로 압축해서 보관하는 방식이다.\n",
        "\n",
        "TFRecordDataset() 메소드는 이러한 TFRecord 형태의 데이터셋을 다룰 수 있도록 하는 메소드다."
      ],
      "metadata": {
        "id": "S5SsHjjwAIv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILENAME = './tf_example/uncompressed_tf_example_validation_validation_tfexample.tfrecord-00000-of-00150'"
      ],
      "metadata": {
        "id": "T-1fNBN_3rap"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzMY0l355nRK",
        "outputId": "7d491884-57ef-48de-cf44-d520898171d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TFRecordDatasetV2 shapes: (), types: tf.string>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for num_data, data in enumerate(dataset):\n",
        "    pass\n",
        "\n",
        "num_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLSrqGqk9kR_",
        "outputId": "6cb3eccb-43d0-40c2-a793-8d14f0e2aa1e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "332"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하나의 tfrecord 파일 내부에는 332개의 데이터가 있는 것을 알 수 있다."
      ],
      "metadata": {
        "id": "H3JGZYd79wIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 파싱"
      ],
      "metadata": {
        "id": "RYs2WnudIb1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 구조를 선언하고, 파싱한다."
      ],
      "metadata": {
        "id": "cV0_om89If5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example field definition\n",
        "roadgraph_features = {\n",
        "    'roadgraph_samples/dir':\n",
        "        tf.io.FixedLenFeature([30000, 3], tf.float32, default_value=None),\n",
        "    'roadgraph_samples/id':\n",
        "        tf.io.FixedLenFeature([30000, 1], tf.int64, default_value=None),\n",
        "    'roadgraph_samples/type':\n",
        "        tf.io.FixedLenFeature([30000, 1], tf.int64, default_value=None),\n",
        "    'roadgraph_samples/valid':\n",
        "        tf.io.FixedLenFeature([30000, 1], tf.int64, default_value=None),\n",
        "    'roadgraph_samples/xyz':\n",
        "        tf.io.FixedLenFeature([30000, 3], tf.float32, default_value=None),\n",
        "}\n",
        "\n",
        "# Features of other agents.\n",
        "state_features = {\n",
        "    'state/id':\n",
        "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
        "    'state/type':\n",
        "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
        "    'state/is_sdc':\n",
        "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
        "    'state/tracks_to_predict':\n",
        "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
        "    'state/current/bbox_yaw':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/height':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/length':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/timestamp_micros':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
        "    'state/current/valid':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
        "    'state/current/vel_yaw':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/velocity_x':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/velocity_y':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/width':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/x':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/y':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/current/z':\n",
        "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    'state/future/bbox_yaw':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/height':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/length':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/timestamp_micros':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
        "    'state/future/valid':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
        "    'state/future/vel_yaw':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/velocity_x':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/velocity_y':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/width':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/x':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/y':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/future/z':\n",
        "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    'state/past/bbox_yaw':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/height':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/length':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/timestamp_micros':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
        "    'state/past/valid':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
        "    'state/past/vel_yaw':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/velocity_x':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/velocity_y':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/width':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/x':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/y':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    'state/past/z':\n",
        "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "}\n",
        "\n",
        "traffic_light_features = {\n",
        "    'traffic_light_state/current/state':\n",
        "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
        "    'traffic_light_state/current/valid':\n",
        "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
        "    'traffic_light_state/current/x':\n",
        "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
        "    'traffic_light_state/current/y':\n",
        "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
        "    'traffic_light_state/current/z':\n",
        "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
        "    'traffic_light_state/past/state':\n",
        "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
        "    'traffic_light_state/past/valid':\n",
        "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
        "    'traffic_light_state/past/x':\n",
        "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
        "    'traffic_light_state/past/y':\n",
        "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
        "    'traffic_light_state/past/z':\n",
        "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
        "}\n",
        "\n",
        "features_description = {}\n",
        "features_description.update(roadgraph_features)\n",
        "features_description.update(state_features)\n",
        "features_description.update(traffic_light_features)"
      ],
      "metadata": {
        "id": "H-AbYLC-ItHl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILENAME = './tf_example/uncompressed_tf_example_validation_validation_tfexample.tfrecord-00000-of-00150'\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
        "data = next(dataset.as_numpy_iterator())\n",
        "parsed = tf.io.parse_single_example(data, features_description)"
      ],
      "metadata": {
        "id": "99aD8RRdJyj6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시각화 이미지 생성"
      ],
      "metadata": {
        "id": "en6NgFe8BFbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "sZaXmzuCTROJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Painter:\n",
        "  \"\"\" 데이터 시각화에 matplotlib를 이용하기 위해 일부 과정 함수화 \"\"\"\n",
        "\n",
        "  def __init__(self, debug=False):\n",
        "    if debug:\n",
        "      fig, ax = self.create_figure_and_axes(400)\n",
        "      self.fig_canvas_image(fig)\n",
        "\n",
        "  def create_figure_and_axes(self, size_pixels=1000):\n",
        "    \"\"\" figure, axes 초기화.\"\"\"\n",
        "    # figure와 axes 객체 생성\n",
        "    fig, ax = plt.subplots(1, 1, num=uuid.uuid4())\n",
        "\n",
        "    # 픽셀로 설정할 수 있도록 변경\n",
        "    dpi = 100\n",
        "    size_inches = size_pixels / dpi\n",
        "    fig.set_size_inches([size_inches, size_inches])\n",
        "    fig.set_dpi(dpi)\n",
        "\n",
        "    # 배경색 흰색으로 설정\n",
        "    fig.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # x,y축 라벨과 선 색상을 검은색으로 설정\n",
        "    ax.xaxis.label.set_color('black')\n",
        "    ax.tick_params(axis='x', colors='black')\n",
        "    ax.yaxis.label.set_color('black')\n",
        "    ax.tick_params(axis='y', colors='black')\n",
        "\n",
        "    # subplot들 간 간격을 자동으로 조절\n",
        "    fig.set_tight_layout(True)\n",
        "\n",
        "    # 그리드 안보이도록 설정\n",
        "    ax.grid(False)\n",
        "    \n",
        "    return fig, ax\n",
        "\n",
        "  def fig_canvas_image(self, fig):\n",
        "    \"\"\" fig를 이미지로 변환\n",
        "    \n",
        "    fig.canvas.tostring_rgb()를 이용하여 [H, W, 3] uint8 np.array 이미지를 반환\n",
        "    \"\"\"\n",
        "\n",
        "    # xticks와 yticks를 표시하기에 충분한 마진 부여\n",
        "    fig.subplots_adjust(\n",
        "        left=0.08, bottom=0.08, right=0.98, top=0.98, wspace=0.0, hspace=0.0)\n",
        "    \n",
        "    # canvas에 그림 그리기\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    # np.frombuffer 함수로 이미지 데이터를 uint8 형태로 변환\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "\n",
        "    # 데이터를 (높이, 너비, 채널 수) 형태로 변경하여 반환\n",
        "    return data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "Painter(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "t_u9qu4fVkmi",
        "outputId": "7802d1d0-0ea4-4d9d-c4a8-8ce738096a36"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Painter at 0x7fb19463e6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VizDatasetProcessor:\n",
        "  def __init__(self, decoded_example, debug=False):\n",
        "    self.raw_data = decoded_example\n",
        "\n",
        "    self.set_seperate_states() # state를 과거/현재/미래로 분리\n",
        "    self.set_other_variables() # 기타 정보들을 정리 및 변수 설정\n",
        "    \n",
        "  def set_seperate_states(self):\n",
        "    \"\"\" state를 과거/현재/미래로 분리 \"\"\"\n",
        "\n",
        "    # 과거 state 데이터 : [num_agents, num_past_steps, 2] float32.\n",
        "    self.past_states = tf.stack(\n",
        "        [self.raw_data['state/past/x'],\n",
        "         self.raw_data['state/past/y']],\n",
        "        -1).numpy()\n",
        "    self.past_states_mask = self.raw_data['state/past/valid'].numpy() > 0.0\n",
        "\n",
        "    # 현재 state 데이터 : [num_agents, 1, 2] float32.\n",
        "    self.current_states = tf.stack(\n",
        "        [self.raw_data['state/current/x'],\n",
        "         self.raw_data['state/current/y']],\n",
        "        -1).numpy()\n",
        "    self.current_states_mask = self.raw_data['state/current/valid'].numpy() > 0.0\n",
        "\n",
        "    # 미래 state 데이터 : [num_agents, num_future_steps, 2] float32.\n",
        "    self.future_states = tf.stack(\n",
        "        [self.raw_data['state/future/x'],\n",
        "         self.raw_data['state/future/y']],\n",
        "        -1).numpy()\n",
        "    self.future_states_mask = self.raw_data['state/future/valid'].numpy() > 0.0\n",
        "  \n",
        "  def set_other_variables(self):\n",
        "    \"\"\" 기타 정보들을 정리 및 변수 설정 \"\"\"  \n",
        "\n",
        "    # 도로 정보\n",
        "    self.roadgraph_xyz = self.raw_data['roadgraph_samples/xyz'].numpy()\n",
        "\n",
        "    # agent 수, 과거 step 수, 미래 step 수\n",
        "    self.num_agents, self.num_past_steps, _ = self.past_states.shape\n",
        "    self.num_future_steps = self.future_states.shape[1]\n",
        "\n",
        "    # agent 별 색상값 설정 \n",
        "    self.color_map = self._get_colormap(self.num_agents)\n",
        "\n",
        "    # 전체 데이터의 중심점 구하기\n",
        "    self.center_y, self.center_x, self.width = self._get_viewport()\n",
        "  \n",
        "  def _get_viewport(self):\n",
        "    \"\"\" 전체 데이터의 중심점 구하기\n",
        "\n",
        "    Returns:\n",
        "      center_y: float. y coordinate for center of data.\n",
        "      center_x: float. x coordinate for center of data.\n",
        "      width: float. Width of data.\n",
        "    \"\"\"\n",
        "    # 모든 state 배열\n",
        "    # : [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
        "    all_states = np.concatenate([\n",
        "        self.past_states,\n",
        "        self.current_states,\n",
        "        self.future_states], 1)\n",
        "    # mask\n",
        "    # : [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
        "    all_states_mask = np.concatenate([\n",
        "        self.past_states_mask,\n",
        "        self.current_states_mask,\n",
        "        self.future_states_mask], 1)\n",
        "\n",
        "    valid_states = all_states[all_states_mask]\n",
        "    all_y = valid_states[..., 1]\n",
        "    all_x = valid_states[..., 0]\n",
        "\n",
        "    center_y = (np.max(all_y) + np.min(all_y)) / 2\n",
        "    center_x = (np.max(all_x) + np.min(all_x)) / 2\n",
        "\n",
        "    range_y = np.ptp(all_y)\n",
        "    range_x = np.ptp(all_x)\n",
        "\n",
        "    width = max(range_y, range_x)\n",
        "\n",
        "    return center_y, center_x, width\n",
        "\n",
        "  \n",
        "  def _get_colormap(self, num_agents):\n",
        "    \"\"\" agent 개수만큼 랜덤하게 colormap 생성 \"\"\"\n",
        "    colors = cm.get_cmap('jet', num_agents)\n",
        "    colors = colors(range(num_agents))\n",
        "    np.random.shuffle(colors)\n",
        "    return colors"
      ],
      "metadata": {
        "id": "wZ7Qq3K6gDaQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Visualizer:\n",
        "  def __init__(self, decoded_example, size_pixels=400, debug=False):\n",
        "    self.debug = debug\n",
        "    self.size_pixels = size_pixels\n",
        "    self.dataset = VizDatasetProcessor(decoded_example)\n",
        "    self.painter = Painter()\n",
        "    \n",
        "  def run(self):\n",
        "    images = self.get_all_step_images()\n",
        "    if self.debug:\n",
        "      images = images[::5]\n",
        "    self.anim = self.create_animation(images)\n",
        "    return self.anim\n",
        "  \n",
        "  def create_animation(self, images):\n",
        "    \"\"\" Creates a Matplotlib animation of the given images.\n",
        "\n",
        "    Args:\n",
        "      images: A list of numpy arrays representing the images.\n",
        "\n",
        "    Returns:\n",
        "      A matplotlib.animation.Animation.\n",
        "\n",
        "    Usage:\n",
        "      anim = create_animation(images)\n",
        "      anim.save('/tmp/animation.avi')\n",
        "      HTML(anim.to_html5_video())\n",
        "    \"\"\"\n",
        "\n",
        "    plt.ioff()\n",
        "    fig, ax = plt.subplots()\n",
        "    dpi = 100\n",
        "    size_inches = self.size_pixels / dpi\n",
        "    fig.set_size_inches([size_inches, size_inches])\n",
        "    plt.ion()\n",
        "\n",
        "    def animate_func(i):\n",
        "      ax.imshow(images[i])\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "      ax.grid('off')\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate_func, frames=len(images) // 2, interval=100)\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "\n",
        "  def get_all_step_images(self):\n",
        "    \"\"\" 각 steps의 이미지를 리스트로 반환하는 함수 \"\"\"\n",
        "    images = []\n",
        "\n",
        "    # 과거\n",
        "    for i, (s, m) in enumerate(\n",
        "        zip(\n",
        "            np.split(self.dataset.past_states, self.dataset.num_past_steps, 1),\n",
        "            np.split(self.dataset.past_states_mask, self.dataset.num_past_steps, 1))):\n",
        "      im = self._visualize_one_step(s[:, 0], m[:, 0],\n",
        "                              'past: %d' % (self.dataset.num_past_steps - i))\n",
        "      images.append(im)\n",
        "\n",
        "    # 현재\n",
        "    s = self.dataset.current_states\n",
        "    m = self.dataset.current_states_mask\n",
        "\n",
        "    im = self._visualize_one_step(s[:, 0], m[:, 0], 'current')\n",
        "    images.append(im)\n",
        "\n",
        "    # 미래\n",
        "    for i, (s, m) in enumerate(\n",
        "        zip(\n",
        "            np.split(self.dataset.future_states, self.dataset.num_future_steps, 1),\n",
        "            np.split(self.dataset.future_states_mask, self.dataset.num_future_steps, 1))):\n",
        "      im = self._visualize_one_step(s[:, 0], m[:, 0],\n",
        "                              'future: %d' % (i + 1))\n",
        "      images.append(im)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "  def _visualize_one_step(self,\n",
        "                          states,\n",
        "                          mask,\n",
        "                          title):\n",
        "    \"\"\"Generate visualization for a single step.\"\"\"\n",
        "\n",
        "    center_y = self.dataset.center_y\n",
        "    center_x = self.dataset.center_x\n",
        "    width    = self.dataset.width\n",
        "\n",
        "\n",
        "    # Create figure and axes.\n",
        "    fig, ax = self.painter.create_figure_and_axes(size_pixels=self.size_pixels)\n",
        "\n",
        "    # Plot roadgraph.\n",
        "    rg_pts = self.dataset.roadgraph_xyz[:, :2].T\n",
        "    ax.plot(rg_pts[0, :], rg_pts[1, :], 'k.', alpha=1, ms=2)\n",
        "\n",
        "    masked_x = states[:, 0][mask]\n",
        "    masked_y = states[:, 1][mask]\n",
        "    colors = self.dataset.color_map[mask]\n",
        "\n",
        "    # Plot agent current position.\n",
        "    ax.scatter(\n",
        "        masked_x,\n",
        "        masked_y,\n",
        "        marker='o',\n",
        "        linewidths=3,\n",
        "        color=colors,\n",
        "    )\n",
        "\n",
        "    # Title.\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Set axes.  Should be at least 10m on a side and cover 160% of agents.\n",
        "    size = max(10, width * 1.0)\n",
        "    ax.axis([\n",
        "        -size / 2 + center_x, size / 2 + center_x, -size / 2 + center_y,\n",
        "        size / 2 + center_y\n",
        "    ])\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    image = self.painter.fig_canvas_image(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    return image\n",
        "\n"
      ],
      "metadata": {
        "id": "zgJ1JmuxVcIW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = Visualizer(parsed, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD62FADlTU4D",
        "outputId": "6498f2b0-da27-4ae4-9565-d09fbdd8e46b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-902c7c9f4437>:86: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  colors = cm.get_cmap('jet', num_agents)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anim = viz.run()"
      ],
      "metadata": {
        "id": "EsfAfae7VJuI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HTML(viz.anim.to_html5_video()) # 영상 보려면 주석 해제"
      ],
      "metadata": {
        "id": "dOzLU0vXnDvY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple MLP로 학습시켜보기 (with. TF)\n",
        "\n"
      ],
      "metadata": {
        "id": "nY4mZnkqIITR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 자체가 복잡해서 간단한 MLP로는 택도 없지만 일단 모델을 만들어보면서 감을 익혀본다"
      ],
      "metadata": {
        "id": "o804pzTU3qBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "FILENAME = './tf_example/uncompressed_tf_example_validation_validation_tfexample.tfrecord-00000-of-00150'\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
        "data = next(dataset.as_numpy_iterator())"
      ],
      "metadata": {
        "id": "_xMEjzbN6knV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input data 구성하기"
      ],
      "metadata": {
        "id": "sz3T0K9G9oHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tf.io.parse_single_example** 함수로 파싱한 데이터(decoded_example)는 속성별로 정리되어있다. 즉, 하나의 리스트에 `[agent1_feature, agent2_feature, ...]`와 같이 agent 별로 묶여있지 않다.\n",
        "\n",
        "모델에 agent 데이터를 한번에 넣어주기 위해 여러 feature를 **tf.stack**으로 묶되, **axis를 -1로 설정**해서 agent별로 묶이도록 수정해준다."
      ],
      "metadata": {
        "id": "LwVhkM4dKS-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse(value):\n",
        "  \"\"\" 데이터를 파싱해서 Input을 만드는 함수 \"\"\"\n",
        "  decoded_example = tf.io.parse_single_example(value, features_description)\n",
        "\n",
        "  # 모델 별 과거 state\n",
        "  past_states = tf.stack([\n",
        "      decoded_example['state/past/x'], decoded_example['state/past/y'],\n",
        "      decoded_example['state/past/length'], decoded_example['state/past/width'],\n",
        "      decoded_example['state/past/bbox_yaw'],\n",
        "      decoded_example['state/past/velocity_x'],\n",
        "      decoded_example['state/past/velocity_y']\n",
        "  ], -1)\n",
        "\n",
        "  # 모델 별 현재 state\n",
        "  cur_states = tf.stack([\n",
        "      decoded_example['state/current/x'], decoded_example['state/current/y'],\n",
        "      decoded_example['state/current/length'],\n",
        "      decoded_example['state/current/width'],\n",
        "      decoded_example['state/current/bbox_yaw'],\n",
        "      decoded_example['state/current/velocity_x'],\n",
        "      decoded_example['state/current/velocity_y']\n",
        "  ], -1)\n",
        "\n",
        "  # 과거 + 현재 => Input\n",
        "  # shape : [agent 수(128), state 수(10+1==11), feature 수(7)]\n",
        "  input_states = tf.concat([past_states, cur_states], 1)\n",
        "  input_states = input_states[..., :2]\n",
        "\n",
        "  # 모델 별 미래 state\n",
        "  future_states = tf.stack([\n",
        "      decoded_example['state/future/x'], decoded_example['state/future/y'],\n",
        "      decoded_example['state/future/length'],\n",
        "      decoded_example['state/future/width'],\n",
        "      decoded_example['state/future/bbox_yaw'],\n",
        "      decoded_example['state/future/velocity_x'],\n",
        "      decoded_example['state/future/velocity_y']\n",
        "  ], -1)\n",
        "\n",
        "  gt_future_states = tf.concat([past_states, cur_states, future_states], 1)\n",
        "\n",
        "  past_is_valid = decoded_example['state/past/valid'] > 0\n",
        "  current_is_valid = decoded_example['state/current/valid'] > 0\n",
        "  future_is_valid = decoded_example['state/future/valid'] > 0\n",
        "  gt_future_is_valid = tf.concat(\n",
        "      [past_is_valid, current_is_valid, future_is_valid], 1)\n",
        "\n",
        "  # If a sample was not seen at all in the past, we declare the sample as\n",
        "  # invalid.\n",
        "  sample_is_valid = tf.reduce_any(\n",
        "      tf.concat([past_is_valid, current_is_valid], 1), 1)\n",
        "\n",
        "  inputs = {\n",
        "      'input_states': input_states,\n",
        "      'gt_future_states': gt_future_states,\n",
        "      'gt_future_is_valid': gt_future_is_valid,\n",
        "      'object_type': decoded_example['state/type'],\n",
        "      'tracks_to_predict': decoded_example['state/tracks_to_predict'] > 0,\n",
        "      'sample_is_valid': sample_is_valid,\n",
        "  }\n",
        "  return inputs\n",
        "\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(FILENAME)\n",
        "data = next(dataset.as_numpy_iterator())\n",
        "_parse(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a_QEiD4v2it",
        "outputId": "9632ecc5-2abb-41e2-d155-dd86a653f2b1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 11, 7)\n",
            "(128, 11, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_states': <tf.Tensor: shape=(128, 11, 2), dtype=float32, numpy=\n",
              " array([[[ 2.7297551e+03, -1.3338293e+03],\n",
              "         [ 2.7303794e+03, -1.3341000e+03],\n",
              "         [ 2.7310159e+03, -1.3343777e+03],\n",
              "         ...,\n",
              "         [ 2.7350720e+03, -1.3361925e+03],\n",
              "         [ 2.7357754e+03, -1.3365101e+03],\n",
              "         [ 2.7364875e+03, -1.3368364e+03]],\n",
              " \n",
              "        [[ 2.7063877e+03, -1.3184196e+03],\n",
              "         [ 2.7063823e+03, -1.3184048e+03],\n",
              "         [ 2.7063889e+03, -1.3184080e+03],\n",
              "         ...,\n",
              "         [ 2.7062402e+03, -1.3183809e+03],\n",
              "         [ 2.7061956e+03, -1.3183558e+03],\n",
              "         [ 2.7061311e+03, -1.3183352e+03]],\n",
              " \n",
              "        [[ 2.6751304e+03, -1.3161355e+03],\n",
              "         [ 2.6752441e+03, -1.3162169e+03],\n",
              "         [ 2.6753342e+03, -1.3163317e+03],\n",
              "         ...,\n",
              "         [ 2.6761272e+03, -1.3171147e+03],\n",
              "         [ 2.6762427e+03, -1.3172480e+03],\n",
              "         [ 2.6763604e+03, -1.3173755e+03]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00]],\n",
              " \n",
              "        [[-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00]],\n",
              " \n",
              "        [[-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00]]], dtype=float32)>,\n",
              " 'gt_future_states': <tf.Tensor: shape=(128, 91, 7), dtype=float32, numpy=\n",
              " array([[[ 2.7297551e+03, -1.3338293e+03,  4.5093503e+00, ...,\n",
              "          -4.1684660e-01,  6.2084961e+00, -2.7539062e+00],\n",
              "         [ 2.7303794e+03, -1.3341000e+03,  4.5227723e+00, ...,\n",
              "          -4.1137376e-01,  6.2426758e+00, -2.7062988e+00],\n",
              "         [ 2.7310159e+03, -1.3343777e+03,  4.5287991e+00, ...,\n",
              "          -4.1736209e-01,  6.3647461e+00, -2.7770996e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00]],\n",
              " \n",
              "        [[ 2.7063877e+03, -1.3184196e+03,  4.1308489e+00, ...,\n",
              "           2.7102110e+00, -1.2939453e-01,  5.2490234e-02],\n",
              "         [ 2.7063823e+03, -1.3184048e+03,  4.1601014e+00, ...,\n",
              "           2.7170680e+00, -5.3710938e-02,  1.4770508e-01],\n",
              "         [ 2.7063889e+03, -1.3184080e+03,  4.1410627e+00, ...,\n",
              "           2.7152586e+00,  6.5917969e-02, -3.1738281e-02],\n",
              "         ...,\n",
              "         [ 2.6725342e+03, -1.3328762e+03,  3.9980290e+00, ...,\n",
              "           4.1489654e+00, -5.4101562e+00, -8.6804199e+00],\n",
              "         [ 2.6719961e+03, -1.3337400e+03,  3.9844046e+00, ...,\n",
              "           4.1582108e+00, -5.3808594e+00, -8.6376953e+00],\n",
              "         [ 2.6714497e+03, -1.3346262e+03,  3.9762745e+00, ...,\n",
              "           4.1665668e+00, -5.4638672e+00, -8.8623047e+00]],\n",
              " \n",
              "        [[ 2.6751304e+03, -1.3161355e+03,  1.8224218e+00, ...,\n",
              "           5.5721049e+00,  1.4331055e+00, -1.4782715e+00],\n",
              "         [ 2.6752441e+03, -1.3162169e+03,  1.8908031e+00, ...,\n",
              "           5.5728641e+00,  1.1376953e+00, -8.1420898e-01],\n",
              "         [ 2.6753342e+03, -1.3163317e+03,  1.8131216e+00, ...,\n",
              "           5.5684614e+00,  9.0087891e-01, -1.1474609e+00],\n",
              "         ...,\n",
              "         [ 2.6958633e+03, -1.3241080e+03,  1.9003750e+00, ...,\n",
              "          -1.2297301e-01,  3.0517578e+00, -3.5400391e-01],\n",
              "         [ 2.6961726e+03, -1.3241471e+03,  1.8828129e+00, ...,\n",
              "          -1.0580855e-01,  3.0932617e+00, -3.9062500e-01],\n",
              "         [ 2.6964775e+03, -1.3241818e+03,  1.9092505e+00, ...,\n",
              "          -9.9761643e-02,  3.0493164e+00, -3.4667969e-01]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00]],\n",
              " \n",
              "        [[-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00]],\n",
              " \n",
              "        [[-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         ...,\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00],\n",
              "         [-1.0000000e+00, -1.0000000e+00, -1.0000000e+00, ...,\n",
              "          -1.0000000e+00, -1.0000000e+00, -1.0000000e+00]]], dtype=float32)>,\n",
              " 'gt_future_is_valid': <tf.Tensor: shape=(128, 91), dtype=bool, numpy=\n",
              " array([[ True,  True,  True, ..., False, False, False],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        [ True,  True,  True, ...,  True,  True,  True],\n",
              "        ...,\n",
              "        [False, False, False, ..., False, False, False],\n",
              "        [False, False, False, ..., False, False, False],\n",
              "        [False, False, False, ..., False, False, False]])>,\n",
              " 'object_type': <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([ 1.,  1.,  3.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,\n",
              "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,  2.,  1.,  1.,  1.,\n",
              "         1.,  1.,  2.,  1.,  2.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
              "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
              "       dtype=float32)>,\n",
              " 'tracks_to_predict': <tf.Tensor: shape=(128,), dtype=bool, numpy=\n",
              " array([ True,  True,  True, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False])>,\n",
              " 'sample_is_valid': <tf.Tensor: shape=(128,), dtype=bool, numpy=\n",
              " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False,  True,  True, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,\n",
              "        False, False])>}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 성능 평가를 위한 Metrics 구성하기"
      ],
      "metadata": {
        "id": "fQQu3WZc9y8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.protobuf import text_format\n",
        "from waymo_open_dataset.protos import motion_metrics_pb2\n",
        "\n",
        "def _default_metrics_config():\n",
        "  config = motion_metrics_pb2.MotionMetricsConfig()\n",
        "  config_text = \"\"\"\n",
        "  track_steps_per_second: 10\n",
        "  prediction_steps_per_second: 2\n",
        "  track_history_samples: 10\n",
        "  track_future_samples: 80\n",
        "  speed_lower_bound: 1.4\n",
        "  speed_upper_bound: 11.0\n",
        "  speed_scale_lower: 0.5\n",
        "  speed_scale_upper: 1.0\n",
        "  step_configurations {\n",
        "    measurement_step: 5\n",
        "    lateral_miss_threshold: 1.0\n",
        "    longitudinal_miss_threshold: 2.0\n",
        "  }\n",
        "  step_configurations {\n",
        "    measurement_step: 9\n",
        "    lateral_miss_threshold: 1.8\n",
        "    longitudinal_miss_threshold: 3.6\n",
        "  }\n",
        "  step_configurations {\n",
        "    measurement_step: 15\n",
        "    lateral_miss_threshold: 3.0\n",
        "    longitudinal_miss_threshold: 6.0\n",
        "  }\n",
        "  max_predictions: 6\n",
        "  \"\"\"\n",
        "  text_format.Parse(config_text, config)\n",
        "  return config"
      ],
      "metadata": {
        "id": "z2t4_YJK7NAu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_config = _default_metrics_config()\n",
        "metrics_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utpyBtYr7aN7",
        "outputId": "b4b734e4-a7b0-43fc-ceae-8343a4d4e919"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "track_steps_per_second: 10\n",
              "prediction_steps_per_second: 2\n",
              "track_history_samples: 10\n",
              "track_future_samples: 80\n",
              "speed_lower_bound: 1.399999976158142\n",
              "speed_upper_bound: 11.0\n",
              "speed_scale_lower: 0.5\n",
              "speed_scale_upper: 1.0\n",
              "step_configurations {\n",
              "  measurement_step: 5\n",
              "  lateral_miss_threshold: 1.0\n",
              "  longitudinal_miss_threshold: 2.0\n",
              "}\n",
              "step_configurations {\n",
              "  measurement_step: 9\n",
              "  lateral_miss_threshold: 1.7999999523162842\n",
              "  longitudinal_miss_threshold: 3.5999999046325684\n",
              "}\n",
              "step_configurations {\n",
              "  measurement_step: 15\n",
              "  lateral_miss_threshold: 3.0\n",
              "  longitudinal_miss_threshold: 6.0\n",
              "}\n",
              "max_predictions: 6"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from waymo_open_dataset.metrics.ops import py_metrics_ops\n",
        "\n",
        "class MotionMetrics(tf.keras.metrics.Metric):\n",
        "  \"\"\"Wrapper for motion metrics computation.\"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self._prediction_trajectory = []\n",
        "    self._prediction_score = []\n",
        "    self._ground_truth_trajectory = []\n",
        "    self._ground_truth_is_valid = []\n",
        "    self._prediction_ground_truth_indices = []\n",
        "    self._prediction_ground_truth_indices_mask = []\n",
        "    self._object_type = []\n",
        "    self._metrics_config = config\n",
        "\n",
        "  def reset_state(self):\n",
        "    self._prediction_trajectory = []\n",
        "    self._prediction_score = []\n",
        "    self._ground_truth_trajectory = []\n",
        "    self._ground_truth_is_valid = []\n",
        "    self._prediction_ground_truth_indices = []\n",
        "    self._prediction_ground_truth_indices_mask = []\n",
        "    self._object_type = []\n",
        "\n",
        "  def update_state(self, prediction_trajectory, prediction_score,\n",
        "                   ground_truth_trajectory, ground_truth_is_valid,\n",
        "                   prediction_ground_truth_indices,\n",
        "                   prediction_ground_truth_indices_mask, object_type):\n",
        "    self._prediction_trajectory.append(prediction_trajectory)\n",
        "    self._prediction_score.append(prediction_score)\n",
        "    self._ground_truth_trajectory.append(ground_truth_trajectory)\n",
        "    self._ground_truth_is_valid.append(ground_truth_is_valid)\n",
        "    self._prediction_ground_truth_indices.append(\n",
        "        prediction_ground_truth_indices)\n",
        "    self._prediction_ground_truth_indices_mask.append(\n",
        "        prediction_ground_truth_indices_mask)\n",
        "    self._object_type.append(object_type)\n",
        "\n",
        "  def result(self):\n",
        "    # [batch_size, num_preds, 1, 1, steps, 2].\n",
        "    # The ones indicate top_k = 1, num_agents_per_joint_prediction = 1.\n",
        "    prediction_trajectory = tf.concat(self._prediction_trajectory, 0)\n",
        "    # [batch_size, num_preds, 1].\n",
        "    prediction_score = tf.concat(self._prediction_score, 0)\n",
        "    # [batch_size, num_agents, gt_steps, 7].\n",
        "    ground_truth_trajectory = tf.concat(self._ground_truth_trajectory, 0)\n",
        "    # [batch_size, num_agents, gt_steps].\n",
        "    ground_truth_is_valid = tf.concat(self._ground_truth_is_valid, 0)\n",
        "    # [batch_size, num_preds, 1].\n",
        "    prediction_ground_truth_indices = tf.concat(\n",
        "        self._prediction_ground_truth_indices, 0)\n",
        "    # [batch_size, num_preds, 1].\n",
        "    prediction_ground_truth_indices_mask = tf.concat(\n",
        "        self._prediction_ground_truth_indices_mask, 0)\n",
        "    # [batch_size, num_agents].\n",
        "    object_type = tf.cast(tf.concat(self._object_type, 0), tf.int64)\n",
        "\n",
        "    # We are predicting more steps than needed by the eval code. Subsample.\n",
        "    interval = (\n",
        "        self._metrics_config.track_steps_per_second //\n",
        "        self._metrics_config.prediction_steps_per_second)\n",
        "    prediction_trajectory = prediction_trajectory[...,\n",
        "                                                  (interval - 1)::interval, :]\n",
        "\n",
        "    return py_metrics_ops.motion_metrics(\n",
        "        config=self._metrics_config.SerializeToString(),\n",
        "        prediction_trajectory=prediction_trajectory,\n",
        "        prediction_score=prediction_score,\n",
        "        ground_truth_trajectory=ground_truth_trajectory,\n",
        "        ground_truth_is_valid=ground_truth_is_valid,\n",
        "        prediction_ground_truth_indices=prediction_ground_truth_indices,\n",
        "        prediction_ground_truth_indices_mask=prediction_ground_truth_indices_mask,\n",
        "        object_type=object_type)"
      ],
      "metadata": {
        "id": "FiW-Kx2G7t9Z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_config = _default_metrics_config()\n",
        "motion_metrics = MotionMetrics(metrics_config)\n",
        "motion_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH-JXv1e7mKK",
        "outputId": "efb2f2b9-1531-48af-c075-c40520b109bd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MotionMetrics at 0x7fb1858f0700>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from waymo_open_dataset.metrics.python import config_util_py as config_util\n",
        "\n",
        "metric_names = config_util.get_breakdown_names_from_motion_config(\n",
        "    metrics_config)\n",
        "\n",
        "metric_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY7bYD199PLB",
        "outputId": "cce1418d-5b0e-4b95-a847-b10e9d9ee28a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TYPE_VEHICLE_5',\n",
              " 'TYPE_VEHICLE_9',\n",
              " 'TYPE_VEHICLE_15',\n",
              " 'TYPE_PEDESTRIAN_5',\n",
              " 'TYPE_PEDESTRIAN_9',\n",
              " 'TYPE_PEDESTRIAN_15',\n",
              " 'TYPE_CYCLIST_5',\n",
              " 'TYPE_CYCLIST_9',\n",
              " 'TYPE_CYCLIST_15']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 만들기"
      ],
      "metadata": {
        "id": "MsF2FLmf9-D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(tf.keras.Model):\n",
        "  \"\"\"A simple one-layer regressor.\"\"\"\n",
        "\n",
        "  def __init__(self, num_agents_per_scenario, num_states_steps,\n",
        "               num_future_steps):\n",
        "    super(SimpleModel, self).__init__()\n",
        "    self._num_agents_per_scenario = num_agents_per_scenario\n",
        "    self._num_states_steps = num_states_steps\n",
        "    self._num_future_steps = num_future_steps\n",
        "    self.regressor = tf.keras.layers.Dense(num_future_steps * 2)\n",
        "\n",
        "  def call(self, states):\n",
        "    states = tf.reshape(states, (-1, self._num_states_steps * 2))\n",
        "    pred = self.regressor(states)\n",
        "    pred = tf.reshape(\n",
        "        pred, [-1, self._num_agents_per_scenario, self._num_future_steps, 2])\n",
        "    return pred\n",
        "\n",
        "model = SimpleModel(\n",
        "          num_agents_per_scenario=128,\n",
        "          num_states_steps=11,\n",
        "          num_future_steps=80)"
      ],
      "metadata": {
        "id": "wjroVU-T-Bjt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 함수 만들기"
      ],
      "metadata": {
        "id": "TPsL0b1v-bjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inputs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # [batch_size, num_agents, D]\n",
        "    states = inputs['input_states']\n",
        "\n",
        "    # Predict. [batch_size, num_agents, steps, 2].\n",
        "    pred_trajectory = model(states, training=True)\n",
        "\n",
        "    # Set training target.\n",
        "    prediction_start = metrics_config.track_history_samples + 1\n",
        "\n",
        "    # [batch_size, num_agents, steps, 7]\n",
        "    gt_trajectory = inputs['gt_future_states']\n",
        "    gt_targets = gt_trajectory[..., prediction_start:, :2]\n",
        "\n",
        "    # [batch_size, num_agents, steps]\n",
        "    gt_is_valid = inputs['gt_future_is_valid']\n",
        "    # [batch_size, num_agents, steps]\n",
        "    weights = (\n",
        "        tf.cast(inputs['gt_future_is_valid'][..., prediction_start:],\n",
        "                tf.float32) *\n",
        "        tf.cast(inputs['tracks_to_predict'][..., tf.newaxis], tf.float32))\n",
        "\n",
        "    loss_value = loss_fn(gt_targets, pred_trajectory, sample_weight=weights)\n",
        "    \n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "  # [batch_size, num_agents, steps, 2] ->\n",
        "  # [batch_size, num_agents, 1, 1, steps, 2].\n",
        "  # The added dimensions are top_k = 1, num_agents_per_joint_prediction = 1.\n",
        "  pred_trajectory = pred_trajectory[:, :, tf.newaxis, tf.newaxis]\n",
        "\n",
        "  # Fake the score since this model does not generate any score per predicted\n",
        "  # trajectory.\n",
        "  pred_score = tf.ones(shape=tf.shape(pred_trajectory)[:3])\n",
        "\n",
        "  # [batch_size, num_agents].\n",
        "  object_type = inputs['object_type']\n",
        "\n",
        "  # [batch_size, num_agents].\n",
        "  batch_size = tf.shape(inputs['tracks_to_predict'])[0]\n",
        "  num_samples = tf.shape(inputs['tracks_to_predict'])[1]\n",
        "\n",
        "  pred_gt_indices = tf.range(num_samples, dtype=tf.int64)\n",
        "  # [batch_size, num_agents, 1].\n",
        "  pred_gt_indices = tf.tile(pred_gt_indices[tf.newaxis, :, tf.newaxis],\n",
        "                            (batch_size, 1, 1))\n",
        "  # [batch_size, num_agents, 1].\n",
        "  pred_gt_indices_mask = inputs['tracks_to_predict'][..., tf.newaxis]\n",
        "\n",
        "  motion_metrics.update_state(pred_trajectory, pred_score, gt_trajectory,\n",
        "                              gt_is_valid, pred_gt_indices,\n",
        "                              pred_gt_indices_mask, object_type)\n",
        "\n",
        "  return loss_value"
      ],
      "metadata": {
        "id": "4lZqJAGT-ekT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습하기"
      ],
      "metadata": {
        "id": "NMHA-Om2-l--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(FILENAME)\n",
        "dataset = dataset.map(_parse)\n",
        "dataset = dataset.batch(32)\n",
        "\n",
        "model = SimpleModel(128, 11, 80)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "epochs = 20\n",
        "num_batches_per_epoch = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('\\nStart of epoch %d' % (epoch,))\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, batch in enumerate(dataset):\n",
        "    loss_value = train_step(batch)\n",
        "\n",
        "    # Log every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print('Training loss (for one batch) at step %d: %.4f' %\n",
        "            (step, float(loss_value)))\n",
        "      print('Seen so far: %d samples' % ((step + 1) * 64))\n",
        "\n",
        "    if step >= num_batches_per_epoch:\n",
        "      break\n",
        "  \n",
        "  continue\n",
        "  # Display metrics at the end of each epoch.\n",
        "  train_metric_values = motion_metrics.result()\n",
        "  for i, m in enumerate(\n",
        "      ['min_ade', 'min_fde', 'miss_rate', 'overlap_rate', 'map']):\n",
        "    for j, n in enumerate(metric_names):\n",
        "      print('{}/{}: {}'.format(m, n, train_metric_values[i, j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vKeHMnU-n4S",
        "outputId": "e7cf83f7-360a-489c-b95d-d86377043671"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 2721793.7500\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 898792.8125\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 2220216.5000\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 711663.1250\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 1835530.2500\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 557186.9375\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 1506777.3750\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 433905.0000\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 1228006.7500\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 337150.0000\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 993993.1875\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 261768.6719\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 799436.5000\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 203213.9375\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 639240.1250\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 157815.2656\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 508635.6562\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 122697.3516\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 403206.6875\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 95613.5469\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss (for one batch) at step 0: 318901.5625\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 74799.4219\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss (for one batch) at step 0: 252069.3281\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 58863.8516\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss (for one batch) at step 0: 199496.3594\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 46709.8281\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss (for one batch) at step 0: 158420.6250\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 37474.8008\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss (for one batch) at step 0: 126518.6719\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 30483.5781\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss (for one batch) at step 0: 101871.4766\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 25210.1777\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss (for one batch) at step 0: 82918.1094\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 21246.6777\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss (for one batch) at step 0: 68404.5938\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 18277.9844\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss (for one batch) at step 0: 57333.4141\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 16061.5967\n",
            "Seen so far: 704 samples\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss (for one batch) at step 0: 48917.1680\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 10: 14411.5312\n",
            "Seen so far: 704 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_metric_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em8Q5IZi7heR",
        "outputId": "abcfd75d-7e20-4f65-a5a7-1975344b5a81"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 9), dtype=float32, numpy=\n",
              "array([[6.7218911e+03, 6.6020376e+03, 6.2157563e+03, 6.5875356e+03,\n",
              "        6.4545649e+03, 6.0878892e+03, 5.3605879e+03, 5.2923281e+03,\n",
              "        4.9972246e+03],\n",
              "       [8.0004863e+03, 5.0152979e+03, 4.9419810e+03, 7.7843931e+03,\n",
              "        4.7506577e+03, 4.5863999e+03, 6.7735498e+03, 4.6196099e+03,\n",
              "        5.1372729e+03],\n",
              "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
              "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
              "        1.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
              "        0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
              "        0.0000000e+00]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "py_metrics_ops.metrics_module.__dir__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "albKHxYOB8Rk",
        "outputId": "b58f953a-a93e-4e1e-a450-4ea9819096d5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__name__',\n",
              " '__doc__',\n",
              " '__package__',\n",
              " '__loader__',\n",
              " '__spec__',\n",
              " '__builtins__',\n",
              " 'collections',\n",
              " 'pywrap_tfe',\n",
              " '_context',\n",
              " '_core',\n",
              " '_execute',\n",
              " '_dtypes',\n",
              " '_op_def_registry',\n",
              " '_ops',\n",
              " '_op_def_library',\n",
              " 'deprecated_endpoints',\n",
              " '_dispatch',\n",
              " 'tf_export',\n",
              " 'TypeVar',\n",
              " '_DetectionMetricsOutput',\n",
              " 'detection_metrics',\n",
              " 'DetectionMetrics',\n",
              " 'detection_metrics_eager_fallback',\n",
              " '_MotionMetricsOutput',\n",
              " 'motion_metrics',\n",
              " 'MotionMetrics',\n",
              " 'motion_metrics_eager_fallback',\n",
              " '_TrackingMetricsOutput',\n",
              " 'tracking_metrics',\n",
              " 'TrackingMetrics',\n",
              " 'tracking_metrics_eager_fallback',\n",
              " '_IS_TENSORFLOW_PLUGIN']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tutorial/2_waymo_official_tutorial/gp-waymo_official_tutorial.ipynb"
      ],
      "metadata": {
        "id": "D30KMw6jILrD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}